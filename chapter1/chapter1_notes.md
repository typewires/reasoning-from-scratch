# Chapter 1: Understanding Reasoning Models

## What is a Reasoning Model?

A reasoning model, also known as reasoning language models (RLMs) or large reasoning models (LRMs), is a type of large language model (LLM) that has been specifically trained to solve complex tasks requiring multiple steps of logical reasoning.

Source: [Wikipedia - Reasoning model](https://en.wikipedia.org/wiki/Reasoning_model)

## LLM Training Pipeline

1. **Pre-training:** Model learns to predict next tokens from vast text data, developing emergent capabilities (translation, code generation, etc.)

2. **Post-training:**
   - *Supervised fine-tuning (SFT) / Instruction tuning:* Teaches the model to follow user instructions
   - *Preference tuning (RLHF):* Improves response quality and tone based on human preferences

3. **Reasoning enhancement:** Applied after conventional training stages

## Three Approaches to Improve Reasoning

1. **Inference-time compute scaling:** Improve reasoning at inference time without modifying model weights. Trade compute for better performance (e.g., chain-of-thought prompting, sampling multiple solutions).

2. **Reinforcement learning:** Train models with reward signals to explicitly improve reasoning.

3. **Distillation:** Transfers reasoning capabilities from larger, more powerful models into smaller, more efficient ones. In practice, this means fine-tuning a smaller model using high-quality instruction datasets generated by a stronger model. This allows smaller models to achieve better reasoning performance without the computational cost of the original large model.

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea28cbec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc484797",
   "metadata": {},
   "source": [
    "# Chapter 4: Improving Reasoning with Inference-Time Scaling\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand inference-time compute scaling as a way to improve accuracy without retraining\n",
    "- Implement chain-of-thought prompting to encourage step-by-step reasoning\n",
    "- Build self-consistency sampling with majority voting\n",
    "- Create flexible text generation with swappable sampling strategies (temperature, top-p)\n",
    "\n",
    "## Core Techniques (notes from the chapter)\n",
    "\n",
    "- Method 1: Extending the chain-of-thought response to prompt the model to explain its reasoning. This is a simple technique that can substantially improve accuracy. \n",
    "- Method 2: Parallel sampling via self-consistency, where the model generates multiple responses and selects the most frequent one. \n",
    "- Method 3: Iterative self-refinement, where the model reviews and improves its own reasoning and answers across multiple steps. (This topic is implemented and covered in more detail in the next chapter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ec3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "âœ“ qwen3/qwen3-0.6B-base.pth already up-to-date\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from reasoning_from_scratch.ch02 import get_device\n",
    "from reasoning_from_scratch.ch03 import (\n",
    "     load_model_and_tokenizer\n",
    ")\n",
    " \n",
    "device = get_device()\n",
    "device = torch.device(\"cpu\")  \n",
    " \n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    which_model=\"base\",\n",
    "    device=device,\n",
    "    use_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b35c55ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful math assistant.\n",
      "Answer the question and write the final result on a new line as:\n",
      "\\boxed{ANSWER}\n",
      "\n",
      "Question:\n",
      "Half the value of $3x-9$ is $x+37$. What is the value of $x$?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from reasoning_from_scratch.ch03 import render_prompt\n",
    " \n",
    "raw_prompt = (\n",
    "    \"Half the value of $3x-9$ is $x+37$. \"\n",
    "    \"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "print(prompt)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_from_scratch.ch02_ex import generate_text_basic_stream_cache\n",
    " \n",
    " \n",
    "def generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt, device, max_new_tokens,\n",
    "    verbose=False, \n",
    "    generate_func=None,  \n",
    "    **generate_kwargs    \n",
    "):\n",
    " \n",
    "    if generate_func is None:  \n",
    "        generate_func = generate_text_basic_stream_cache\n",
    "        \n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt), device=device\n",
    "        ).unsqueeze(0)\n",
    " \n",
    "    generated_ids = []\n",
    "    for token in generate_func(\n",
    "        model=model,\n",
    "        token_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        **generate_kwargs,  \n",
    "    ):\n",
    "        next_token_id = token.squeeze(0)\n",
    "        generated_ids.append(next_token_id.item())\n",
    " \n",
    "        if verbose:\n",
    "            print(\n",
    "                tokenizer.decode(next_token_id.tolist()),\n",
    "                end=\"\",\n",
    "                flush=True\n",
    "            )\n",
    "    return tokenizer.decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5359f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{20}"
     ]
    }
   ],
   "source": [
    "response = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_basic_stream_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d731f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (reasoning-from-scratch)",
   "language": "python",
   "name": "reasoning-from-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

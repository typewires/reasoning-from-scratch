{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def find_repo_root(marker_file=\"requirements.txt\"):\n",
    "    prev, curr = None, os.path.abspath(os.getcwd())\n",
    "    while prev != curr:\n",
    "        if os.path.exists(os.path.join(curr, marker_file)):\n",
    "            return curr\n",
    "        prev, curr = curr, os.path.dirname(curr)\n",
    "    return None\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "if repo_root and repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f7c48",
   "metadata": {},
   "source": [
    "# Chapter 3: Evaluating Reasoning Models\n",
    "\n",
    "  ## Learning Objectives\n",
    "  - Extract and parse final answers from LLM text responses reliably\n",
    "  - Verify answer correctness using symbolic math solvers (calculator-like verification)\n",
    "  - Build an evaluation pipeline: load model → generate outputs → grade against dataset\n",
    "  - Implement verifiable rewards system (foundation for Chapter 6 reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0750dd",
   "metadata": {},
   "source": [
    " <img src=\"figure1.png\" alt=\"Figure 1\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "id": "bec3b9a3",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\n\ndef find_repo_root(marker_file=\"requirements.txt\"):\n    prev, curr = None, os.path.abspath(os.getcwd())\n    while prev != curr:\n        if os.path.exists(os.path.join(curr, marker_file)):\n            return curr\n        prev, curr = curr, os.path.dirname(curr)\n    return None\n\nrepo_root = find_repo_root()\nif repo_root and repo_root not in sys.path:\n    sys.path.insert(0, repo_root)\n\nprint(\"Python executable:\", sys.executable)\nprint(\"Python version:\", sys.version)\nprint(\"Python path:\")\nfor p in sys.path:\n    print(f\"  {p}\")\n\n# Test if the package is available\ntry:\n    import reasoning_from_scratch\n    print(f\"\\nreasoning_from_scratch found at: {reasoning_from_scratch.__file__}\")\nexcept ImportError as e:\n    print(f\"\\nImport error: {e}\")\n\n# Try the specific import\ntry:\n    from reasoning_from_scratch.qwen3 import download_qwen3_small\n    print(\"qwen3 import successful!\")\nexcept ImportError as e:\n    print(f\"qwen3 import failed: {e}\")\n\nfrom pathlib import Path\nimport torch\n\nfrom reasoning_from_scratch.qwen3 import (\n    download_qwen3_small,\n    Qwen3Tokenizer,\n    Qwen3Model,\n    QWEN_CONFIG_06_B\n)\n\ndef load_model_and_tokenizer(\n    which_model, device, use_compile, local_dir=\"qwen3\"\n):\n    if which_model == \"base\":\n\n        download_qwen3_small(\n            kind=\"base\", tokenizer_only=False, out_dir=local_dir\n        )\n\n        tokenizer_path = Path(local_dir) / \"tokenizer-base.json\"\n        model_path = Path(local_dir) / \"qwen3-0.6B-base.pth\"\n        tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)\n\n    elif which_model == \"reasoning\":\n\n        download_qwen3_small(\n            kind=\"reasoning\", tokenizer_only=False, out_dir=local_dir\n        )\n\n        tokenizer_path = Path(local_dir) / \"tokenizer-reasoning.json\"\n        model_path = Path(local_dir) / \"qwen3-0.6B-reasoning.pth\"\n        tokenizer = Qwen3Tokenizer(\n            tokenizer_file_path=tokenizer_path,\n            apply_chat_template=True,\n            add_generation_prompt=True,\n            add_thinking=True,\n        )\n\n    else:\n        raise ValueError(f\"Invalid choice: which_model={which_model}\")\n\n    model = Qwen3Model(QWEN_CONFIG_06_B)\n    model.load_state_dict(torch.load(model_path))\n\n    model.to(device)\n\n    if use_compile:\n        torch._dynamo.config.allow_unspec_int_on_nn_module = True\n        model = torch.compile(model)\n\n    return model, tokenizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16da643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}